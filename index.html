<!DOCTYPE html>
<html>
  <head>
    <title>SMLLE Demos</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    <script src="load_table.js" defer></script>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Open+Sans);
      body {
        font-family: 'Open-Sans', sans-serif;
        font-weight: 300;
        background-color: #fff;
      }
      td {
        vertical-align: middle;
        text-align: justify;
        width: 0vw;
        min-width: 250px;
      }
      audio {
        width: 14vw;
        min-width: 100px;
      }
      .content {
        width: 69vw;
        padding: 25px 50px;
        margin: 25px auto;
        background-color: white;
        box-shadow: 0px 0px 10px #999;
        border-radius: 15px;
        font-family: "Google Sans";
      }
      .contentblock {
        width: 950px;
        margin: 0 auto;
        padding: 0;
        border-spacing: 25px 0;
      }
      .contentblock td {
        background-color: #fff;
        padding: 25px 50px;
        vertical-align: top;
        box-shadow: 0px 0px 10px #999;
        border-radius: 15px;
      }
      a, a:visited {
        color: #224b8d;
        font-weight: 300;
      }
      #authors {
        text-align: center;
        margin-bottom: 20px;
        font-size: 20px;
      }
      #conference {
        text-align: center;
        margin-bottom: 20px;
        font-style: italic;
      }
      #authors a {
        margin: 0 10px;
      }
      h1 {
        text-align: center;
        font-size: 35px;
        font-weight: 300;
      }
      h2 {
        font-size: 30px;
        font-weight: 300;
      }
      h3 {
        font-size: 25px;
        font-weight: 300;
      }
      h4 {
        font-size: 20px;
        font-weight: 300;
      }
      code {
        display: block;
        padding: 10px;
        margin: 10px 10px;
      }
      p {
        line-height: 25px;
        text-align: justify;
      }
      p code {
        display: inline;
        padding: 0;
        margin: 0;
      }
      #teasers {
        margin: 0 auto;
      }
      #teasers td {
        margin: 0 auto;
        text-align: center;
        padding: 5px;
      }
      #teasers img {
        width: 250px;
      }
      #results img {
        width: 133px;
      }
      #seeintodark {
        margin: 0 auto;
      }
      #sift {
        margin: 0 auto;
      }
      #sift img {
        width: 250px;
      }
      .downloadpaper {
        padding-left: 20px;
        float: right;
        text-align: center;
      }
      .downloadpaper a {
        font-weight: bold;
        text-align: center;
      }
      .teaser-img {
        width: 80%;
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .teaser-gif {
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .summary-img {
        width: 100%;
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .video-iframe {
        width: 1000;
        height: 800;
        margin: auto;
        display: block;
      }
      .container {
        display: flex;
        align-items: center;
        justify-content: center
      }
      .image {
        flex-basis: 40%
      }
      .text {
        font-size: 20px;
        padding-left: 20px;
      }
      .center {
        margin-left: auto;
        margin-right: auto;
      }
      .boxshadow {
        border: 1px solid;
        padding: 10px;
        box-shadow: 2px 2px 5px #888888;
      }
      .spacertr {
        height: 8px;
      }
      .spacertd {
        width: 40px;
      }
    </style>
  </head>
  <body>
    <div class="content">
      <div class="text-center">
        <h1><strong>SMLLE: Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling</strong></h1>
        <h5></h5>
        <!-- <span style="font-size: 24px">Anonymous</span></p> -->
        <!-- <p class="lead fw-bold">
          |<a
            href=""
            class="btn border-white bg-white fw-bold"
            >paper</a
          >|
        </p> -->
      </div>
        <!-- <h4 style="text-align:center;">[<a href="#">Paper</a>] [<a href="pdfs/appendix.pdf">Appendix</a>]</h4> -->
        <h2 style="text-align:center;">Abstract</h2>
        <div style="text-align: justify;">
           <!-- With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To alleviate these issues, we propose <b>VALL-E R</b>, which is a robust and efficiency zero-shot TTS system. Specifically, it introduces phoneme monotonic alignment strategy to enhance the connection between phonemes and acoustic sequence. And merge codec is adopt to downsample the discrete codes in shallow layer without affecting the speech quality. Benefit from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth in experimental results. In addition, it requires fewer autoregressive steps during inference, resulting in over 60\% time savings in inference time. <br> -->

           Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete \(\langle Bos \rangle\) Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. <br>

           This page is for <b>research demonstration purposes only</b>.
        </div>
      </p>
    </div>
    <div class="content">
      <h2 id="model-overview" style="text-align: center;">Architecture Overview</h2>
      <body>
      <p style="text-align: center;">
        <img src="picture/SMLLE_framework.png" style="width: 45vw; margin-left: 0vw;">
      </p>
        <div style="text-align: justify;">
          The overview of SMLLE. SMLLE has two key components: (1) A Transducer based streaming model to generate the semantic tokens, based on the text.
          (2) An AR streaming model to generate the mel spectrum sequence for the final speech reconstruction, based on the semantic token sequence generated by the first component and the duration-aligned text with the duration information from the Transducer model.
        </div>
      </body>
    </div>


    <div class="content">
      <h2 style="text-align: center;">Samples for Cross-Sentence Voice Clone</h2>
      <div style="text-align: justify;">
        <b>Text</b>: Text transcription. <br>
        <b>GroundTruth</b>: The acoustic groundtruth. <br>
        <b>SMLLE-Transducer</b>: Speech waveform generated by proposed SMLLE Transducer Stage. <br>
        <b>SMLLE-Mel AR</b>: Speech waveform generated by proposed SMLLE Autoregressive Stage. <br>
        <b>MELLE</b>: Speech waveform generated by baseline MELLE. <br>
        <b>VALL-E</b>: Speech waveform generated by baseline VALL-E. <br>
        <b>YourTTS</b>: Speech waveform generated by baseline YourTTS. <br>
      </div>
      <div style="margin-top: 0vh; text-align: center;">
        <div class="table-responsive pt-3">
          <ul class="pagination justify-content-center">
            <li class="page-item active">
              <a id="cross-1" class="page-link" href="#">1</a>
            </li>
            <!-- <li class="page-item">
              <a id="cross-2" class="page-link" href="#">2</a>
            </li> -->
          </ul>
          <table class="table pt-2" id="cross">
            <thead>
              <tr>
                <th>Text</th>
                <th style="text-align: center">GroundTruth</th>
                <th style="text-align: center">SMLLE-Transducer</th>
                <th style="text-align: center">SMLLE-Mel AR</th>
                <th style="text-align: center">MELLE</th>
                <th style="text-align: center">VALL-E</th>
                <th style="text-align: center">YourTTS</th>
              </tr>
            </thead>
            <tbody></tbody>
          </table>
        </div>
      </div>
    </div>

    <div class="content">
      <h2 style="text-align: center;">Interactive Speech Generation</h2>
      <div style="text-align: justify; margin-bottom: 30px;">
        Move the slider or click the text to simulate the streaming generation process. The speech will update automatically as you adjust the slider position. Due to the presence of DBM, the SMLLE will not trigger the AR Stage when only <bos> is input, and at this point, the Zero-shot Streaming Speech does not exist.
      </div>
      
      <!-- Placeholder for dynamic content -->
      <div id="dynamicContent"></div>
      
      <script>
        function generateInteractiveContent(textLists, audioBasePaths) {
          // Create the container for dynamic content
          const contentContainer = document.getElementById("dynamicContent");
          contentContainer.innerHTML = ""; // Clear any existing content
          
          // Iterate through the provided textLists and audioBasePaths
          textLists.forEach((textList, index) => {
            const audioBasePath = audioBasePaths[index];
            
            // Create the text container dynamically for each textList
            const textContainer = document.createElement("div");
            textContainer.style.color = "#666";
            textContainer.style.lineHeight = "1.8";
            textContainer.style.padding = "20px";
            textContainer.style.background = "#f8f9fa";
            textContainer.style.borderRadius = "10px";
            textContainer.style.display = "flex";
            textContainer.style.justifyContent = "space-between";
            textContainer.id = `text-container-${index}`;
            
            // Add text spans dynamically based on the textList
            textList.forEach((word, wordIndex) => {
              const wordElement = document.createElement("span");
              wordElement.id = `text-${index}-${wordIndex}`;
              wordElement.textContent = word;
              
              // 给单词添加光标样式，鼠标悬停时显示点击光标
              wordElement.style.cursor = "pointer";
              
              // Add click event to each word to update slider position
              wordElement.addEventListener("click", () => {
                const slider = document.getElementById(`speechSlider-${index}`);
                slider.value = wordIndex;  // Update slider value to clicked word index
                slider.dispatchEvent(new Event("input"));  // Trigger slider input event to update speech
              });
              
              textContainer.appendChild(wordElement);
            });

            
            contentContainer.appendChild(textContainer);
            
            // Create the slider for each textList
            const sliderContainer = document.createElement("div");
            sliderContainer.style.position = "relative";
            sliderContainer.style.margin = "20px auto";
            sliderContainer.style.width = "95%";
            
            const slider = document.createElement("input");
            slider.type = "range";
            slider.id = `speechSlider-${index}`;
            slider.min = "0";
            slider.max = `${textList.length - 1}`;
            slider.value = "0";
            slider.step = "1";
            slider.style.width = "100%";
            slider.style.height = "15px";
            slider.style.webkitAppearance = "none";
            slider.oninput = function () {
              updateSpeech(index, this.value, textList, audioBasePath);
            };
            
            sliderContainer.appendChild(slider);
            contentContainer.appendChild(sliderContainer);
            
            // Audio player container
            const audioContainer = document.createElement("div");
            audioContainer.style.textAlign = "center";
            audioContainer.style.marginTop = "20px";
            
             // Add audio name before the audio player
            const audioName1 = document.createElement("p");
            audioName1.textContent = `Speech ${index+1}: The Semantic Speech generated by the Transducer Stage.`; // Add the name for the first audio
            audioName1.style.fontSize = "14px";
            audioName1.style.fontWeight = "bold";
            audioName1.style.marginBottom = "5px";
            
            const audioName2 = document.createElement("p");
            audioName2.textContent = `Speech ${index+1}: The Zero-shot Streaming Speech generated by the AR Stage.`; // Add the name for the second audio
            audioName2.style.fontSize = "14px";
            audioName2.style.fontWeight = "bold";
            audioName2.style.marginBottom = "5px";

            // First audio player
            const audioPlayer1 = document.createElement("audio");
            audioPlayer1.id = `dynamicAudio-${index}-1`;
            audioPlayer1.controls = true;
            audioPlayer1.style.width = "80%";
            const audioSource1 = document.createElement("source");
            audioSource1.src = `${audioBasePath}/step_0_phone_st.wav`; // Set the initial audio file
            audioSource1.type = "audio/wav";
            audioPlayer1.appendChild(audioSource1);
            
            // Second audio player
            const audioPlayer2 = document.createElement("audio");
            audioPlayer2.id = `dynamicAudio-${index}-2`;
            audioPlayer2.controls = true;
            audioPlayer2.style.width = "80%";
            const audioSource2 = document.createElement("source");
            audioSource2.src = `${audioBasePath}/step_0_phone_mel.wav`; // Add the second audio file
            audioSource2.type = "audio/wav";
            audioPlayer2.appendChild(audioSource2);
            
            // Append both audio players to the audio container
            audioContainer.appendChild(audioName1); // Add the first audio name
            audioContainer.appendChild(audioPlayer1); // Add the first audio player
            audioContainer.appendChild(audioName2); // Add the second audio name
            audioContainer.appendChild(audioPlayer2); // Add the second audio player
            contentContainer.appendChild(audioContainer);
            
            // Initialize with the first text and audio
            updateSpeech(index, 0, textList, audioBasePath);
          });
        }


        
        function updateSpeech(index, position, textList, audioBasePath) {
          const totalWords = textList.length; // 动态获取文本片段数量

          // Update text highlighting
          for (let i = 0; i < totalWords; i++) {
            const wordElement = document.getElementById(`text-${index}-${i}`);
            if (i <= position) {
              // Words before and at the slider position
              wordElement.style.color = '#224b8d'; // 高亮颜色
              wordElement.style.fontWeight = 'bold'; // 加粗
            } else {
              // Words after the slider position
              wordElement.style.color = '#999'; // 灰色
              wordElement.style.fontWeight = 'normal'; // 正常字体
            }
          }
          
          // Update the audio file based on the slider position
          const audioPlayer = document.getElementById(`dynamicAudio-${index}-1`);
          const audioSource = audioPlayer.querySelector("source");
          audioSource.src = `${audioBasePath}/step_${position}_phone_st.wav`; // Update with the new audio file
          audioPlayer.load();  // Reload the audio player to apply the new source
          // audioPlayer.play();  // Play the updated audio immediately
          
          // Optionally, you can also update the second audio file if needed
          const audioPlayer2 = document.getElementById(`dynamicAudio-${index}-2`);
          const audioSource2 = audioPlayer2.querySelector("source");
          audioSource2.src = `${audioBasePath}/step_${position}_phone_mel.wav`; // Update the second audio file if necessary
          audioPlayer2.load();  // Reload the second audio player to apply the new source
        }
        
        // Example usage with dynamic data
        const textLists = [
          ["<bos>", "oʊ", "_", "j", "uː", "_", "ɑːɹ", "ð", "ə", "_", "d", "ɪ", "ɹ", "ɪ", "s", "t", "æ", "d", "b", "ɛ", "s", "m", "ɪ", "s", "t", "ɚ", "k", "ɪ", "ŋ", "aɪ", "_", "ɛ", "v", "ɚ", "s", "ɔː", "_", "b", "ʌ", "t", "h", "aʊ", "d", "ɪ", "d", "j", "uː", "m", "eɪ", "k", "m", "æ", "m", "i", "l", "ɛ", "t", "ɜː", "k", "ʌ", "m", "<eos>"],
          ["<bos>", "æ", "z", "f", "ɚ", "ð", "ɪ", "_", "ɪ", "k", "θ", "ɪ", "ə", "s", "ɔː", "ɹ", "ə", "s", "h", "æ", "z", "h", "iː", "ɹ", "ᵻ", "t", "ɜː", "n", "d", "t", "uː", "h", "ɪ", "z", "s", "ʌ", "b", "m", "ɚ", "ɹ", "iː", "n", "k", "æ", "v", "ɚ", "n", "<eos>"],
          ["<bos>", "æ", "t", "iː", "t", "aɪ", "m", "_", "ð", "eɪ", "w", "ɜː", "s", "æ", "d", "æ", "n", "d", "s", "aɪ", "l", "ə", "n", "t", "_", "æ", "n", "ð", "ə", "m", "iː", "l", "w", "ɛ", "n", "t", "_", "ɐ", "w", "eɪ", "_", "ʌ", "n", "t", "ʌ", "tʃ", "t", "b", "aɪ", "_", "ɛ", "n", "i", "ʌ", "v", "ð", "ə", "θ", "ɹ", "iː", "<eos>"],
          ["<bos>", "æ", "n", "d", "l", "eɪ", "m", "iː", "d", "aʊ", "n", "ɪ", "n", "ð", "aɪ", "_", "k", "oʊ", "l", "d", "_", "b", "ɛ", "d", "_", "æ", "n", "d", "l", "iː", "v", "m", "aɪ", "_", "ʃ", "aɪ", "n", "ɪ", "ŋ", "l", "ɑː", "t", "<eos>"],
          ["<bos>", "j", "uː", "_", "ɔː", "t", "uː", "n", "oʊ", "dʒ", "ɑː", "n", "ɪ", "f", "aɪ", "_", "t", "iː", "tʃ", "n", "iː", "ɡ", "ɹ", "oʊ", "z", "_", "aɪ", "l", "s", "k", "ɛɹ", "s", "l", "i", "s", "iː", "_", "m", "ʌ", "tʃ", "ʌ", "v", "p", "iː", "p", "əl", "ɪ", "n", "m", "aɪ", "_", "oʊ", "n", "k", "l", "æ", "s", "<eos>"],
        ];
        const audioBasePaths = [
          "smlle_cross/237-126133-0010_split_for_every_phone",
          "smlle_cross/260-123286-0031_split_for_every_phone",
          "smlle_cross/3575-170457-0051_split_for_every_phone",
          "smlle_cross/908-157963-0027_split_for_every_phone",
          "smlle_cross/1995-1826-0009_split_for_every_phone",
        ]; // Base paths for audio files
        
        // Generate the interactive content with the provided text and audio
        document.addEventListener("DOMContentLoaded", () => {
          generateInteractiveContent(textLists, audioBasePaths);
        });
      </script>
    </div>

    <div class="content">
      <h2 id="model-overview" style="text-align: center;">Ethics Statement</h2>
      <div style="text-align: justify;">
        Since SMLLE could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model.
      </div>
    </div>
    
    <!-- <p style="text-align: center;">
      <img src="https://badges.toozhao.com/badges/01HW75WW0HVJR8Q66FJKQ7ZE07/green.svg" />
    </p> -->
    </body>

  </body>
</html>

